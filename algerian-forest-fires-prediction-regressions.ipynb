{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Problem statement\nThe problem we aim to solve here is predicting the Fire Weather Index (FWI) for the Bejaia and Sidi Bel-abbes regions in Algeria. The FWI is an important measure of fire danger, which indicates the likelihood of forest fires based on weather conditions. By using regression algorithms, we want to build a mathematical model that can understand how different weather factors (like temperature, humidity, wind speed, and rain) and FWI components (FFMC, DMC, DC, ISI, BUI) influence the FWI.\n\nTo achieve this, we will use the available historical data to train the regression models. This data contains information about weather conditions and corresponding FWI values for various days from June to September in 2012. Once the models are trained, we can use them to make predictions about the FWI for future dates based on the expected weather conditions.\n\nThe ultimate aim is to have accurate models that can help us predict the Fire Weather Index, which can be valuable for fire management and prevention strategies in these regions of Algeria.","metadata":{"id":"Pq6LihvIHXWZ","execution":{"iopub.status.busy":"2023-09-14T07:25:00.406301Z","iopub.execute_input":"2023-09-14T07:25:00.406821Z","iopub.status.idle":"2023-09-14T07:25:00.414731Z","shell.execute_reply.started":"2023-09-14T07:25:00.406779Z","shell.execute_reply":"2023-09-14T07:25:00.413467Z"}}},{"cell_type":"markdown","source":"## Dataset information\nThe dataset includes 244 instances that regroup a data of two regions of Algeria,namely the Bejaia region located in the northeast of Algeria and the Sidi Bel-abbes region located in the northwest of Algeria.\n\n122 instances for each region.\n\nThe period from June 2012 to September 2012. The dataset includes 11 attribues and 1 output attribue (class) The 244 instances have been classified into fire(138 classes) and not fire (106 classes) classes.\n\nDataset columns:\n\n**Date** : (DD/MM/YYYY) Day, month ('june' to 'september'), year (2012) Weather data observations\n\n**Temp** : temperature noon (temperature max) in Celsius degrees: 22 to 42\n\n**RH** : Relative Humidity in %: 21 to 90\n\n**Ws** :Wind speed in km/h: 6 to 29\n\n**Rain**: total day in mm: 0 to 16.8 FWI Components\n\n**Fine Fuel Moisture Code (FFMC)** index from the FWI system: 28.6 to 92.5\n\n**Duff Moisture Code (DMC)** index from the FWI system: 1.1 to 65.9\n\n**Drought Code (DC)** index from the FWI system: 7 to 220.4\n\n**Initial Spread Index (ISI)** index from the FWI system: 0 to 18.5\n\n**Buildup Index (BUI)** index from the FWI system: 1.1 to 68\n\n**Fire Weather Index (FWI)** Index: 0 to 31.1\n\n**Classes**: two classes, namely Fire and not Fire\n\n","metadata":{}},{"cell_type":"markdown","source":"**I have already completed the Exploratory Data Analysis (EDA) and feature engineering in my previous notebook. Now, I am using the cleaned dataset from that notebook to make predictions and solve the given problem.** \n\nYou can download the cleaned or processed dataset from the provided link below.\n\n### Algerian Forest Fires Processed Dataset\n- Algerian Forest Fires Processed Dataset :üëâ  **[Link](https://www.kaggle.com/datasets/sudhanshu432/algerian-forest-fires-cleaned-dataset)**\n\n### Notebook for Exploratory Data Analysis and Feature Engineering Applied On Algerian Forest Fires Dataset\n- Here is the notebook we used earlier to analyze and improve the Algerian Forest Fires dataset through Exploratory Data Analysis (EDA) and Feature Engineering (FE) :üëâ **[Link](https://www.kaggle.com/code/sudhanshu432/eda-and-fe-algerian-forest-fires-dataset)**\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:15:32.786859Z","iopub.execute_input":"2023-09-14T09:15:32.787391Z","iopub.status.idle":"2023-09-14T09:15:32.794005Z","shell.execute_reply.started":"2023-09-14T09:15:32.787356Z","shell.execute_reply":"2023-09-14T09:15:32.79286Z"}}},{"cell_type":"markdown","source":"## Regression\nRegression is a valuable and widely used tool in the world of data science and machine learning. It empowers us to explore and predict the connections between multiple factors. In simpler terms, regression allows us to uncover a mathematical equation that links one factor (the thing we want to figure out) with one or more other factors (the things we think influence it).\n\nThink about having a bag full of various fruits, and you're curious about how the weight of a fruit is related to its size. With regression, you can discover a formula that explains how the weight changes when the size varies. This formula becomes your guide for making predictions about a fruit's weight based on its size. In essence, regression helps us unravel the mysteries hidden within our data and make informed forecasts.\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## The Importance of Regression Analysis\n\nImagine you're in a neighborhood where you know the prices of houses, and you're eyeing a new house. You're curious about how much that house should cost based on its characteristics like size, the number of rooms, and where it's located. That's where regression comes into play. It's like having a crystal ball that can predict the house's price for you.\n\nBut regression isn't just for house hunting; it's a powerful tool for uncovering connections between things. Let's say you're a student wondering if the time you spend studying has a real impact on your exam scores. Regression can dig into your study habits and show you if there's a strong link between hitting the books and acing those tests.\n\nIn the real world, things get complicated. There are tons of variables at play, and it's not always clear which ones truly matter. Regression helps cut through the confusion. It's like being a detective, finding the important clues in a sea of information. It helps businesses, like your favorite store, predict how much of a product they'll need so you never run out of your must-haves.\n\nIn a nutshell, regression is your trusty sidekick when you want to understand how things are connected, make predictions, and make smart decisions based on data. It's the tool that guides you through the maze of information in our data-driven world.\n\nIn summary, regression is a valuable tool for\n1. **Prediction:** Let's say you have information about the price of houses in a neighborhood and want to know the price of a new house. Regression helps you make a prediction based on the features of the new house, such as its size, number of rooms, and location.\n2. **Understanding Relationships :** Regression helps us understand how different factors influence each other. For example, you might want to know how the amount of time spent studying affects exam scores. Regression can show you if there is a strong relationship between study time and scores.\n3. **Identifying Important Factors:** In complex situations with many variables, regression helps us figure out which factors have a significant impact on the outcome. It helps us separate the essential factors from the ones that don't matter much.\n4. **Decision Making:** Organizations and businesses use regression to make informed decisions. For instance, a company might use regression to predict customer demand for a product, helping them plan their production and inventory efficiently.\n\n\n--------------------------------------------------------------------------------or------------------------------------------------------------------------\n\nRegression is a valuable tool for understanding relationships between variables and making predictions. Its need arises whenever we want to understand, predict, or make decisions based on data and the relationships between different factors.\n\n **********************************************************************************************************************************************************","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-09-14T10:02:47.693641Z","iopub.execute_input":"2023-09-14T10:02:47.695346Z","iopub.status.idle":"2023-09-14T10:02:47.705134Z","shell.execute_reply.started":"2023-09-14T10:02:47.695288Z","shell.execute_reply":"2023-09-14T10:02:47.702821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/algerian-forest-fires-cleaned-dataset/Algerian_forest_fires_cleaned_dataset.csv')","metadata":{"id":"ep32_WS3HXWk","execution":{"iopub.status.busy":"2023-09-14T10:02:47.742961Z","iopub.execute_input":"2023-09-14T10:02:47.743403Z","iopub.status.idle":"2023-09-14T10:02:47.762107Z","shell.execute_reply.started":"2023-09-14T10:02:47.743365Z","shell.execute_reply":"2023-09-14T10:02:47.760437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"uaK-HmQRHXWl","outputId":"ff239d4d-e8ef-4f98-9cba-f8eb8d6af08b","execution":{"iopub.status.busy":"2023-09-14T10:02:47.825284Z","iopub.execute_input":"2023-09-14T10:02:47.826619Z","iopub.status.idle":"2023-09-14T10:02:47.848124Z","shell.execute_reply.started":"2023-09-14T10:02:47.826567Z","shell.execute_reply":"2023-09-14T10:02:47.846965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"id":"N0allNEBHXWn","outputId":"3350d2ba-bd47-4c04-dd45-b24c3016819d","execution":{"iopub.status.busy":"2023-09-14T10:02:47.908141Z","iopub.execute_input":"2023-09-14T10:02:47.908659Z","iopub.status.idle":"2023-09-14T10:02:47.918154Z","shell.execute_reply.started":"2023-09-14T10:02:47.908621Z","shell.execute_reply":"2023-09-14T10:02:47.916554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##drop month,day and yyear\ndf.drop(['day','month','year'],axis=1,inplace=True)","metadata":{"id":"uI-vEePTHXWo","execution":{"iopub.status.busy":"2023-09-14T10:02:47.976503Z","iopub.execute_input":"2023-09-14T10:02:47.977571Z","iopub.status.idle":"2023-09-14T10:02:47.985983Z","shell.execute_reply.started":"2023-09-14T10:02:47.977532Z","shell.execute_reply":"2023-09-14T10:02:47.984529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"f7_az-7aHXWp","outputId":"3ddd924b-976b-4918-d975-ff18eb117d7e","execution":{"iopub.status.busy":"2023-09-14T10:02:48.04936Z","iopub.execute_input":"2023-09-14T10:02:48.050865Z","iopub.status.idle":"2023-09-14T10:02:48.074272Z","shell.execute_reply.started":"2023-09-14T10:02:48.050802Z","shell.execute_reply":"2023-09-14T10:02:48.072961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Classes'].value_counts()","metadata":{"id":"2oUhXL9KHXWq","outputId":"d804f233-e2a4-4dd4-e7ad-dc7848c577fc","execution":{"iopub.status.busy":"2023-09-14T10:02:48.122796Z","iopub.execute_input":"2023-09-14T10:02:48.1238Z","iopub.status.idle":"2023-09-14T10:02:48.135583Z","shell.execute_reply.started":"2023-09-14T10:02:48.123746Z","shell.execute_reply":"2023-09-14T10:02:48.133753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding","metadata":{}},{"cell_type":"code","source":"## Encoding\ndf['Classes']=np.where(df['Classes'].str.contains(\"not fire\"),0,1)","metadata":{"id":"j2F-NF2bHXWq","execution":{"iopub.status.busy":"2023-09-14T10:02:48.180378Z","iopub.execute_input":"2023-09-14T10:02:48.180893Z","iopub.status.idle":"2023-09-14T10:02:48.188662Z","shell.execute_reply.started":"2023-09-14T10:02:48.180859Z","shell.execute_reply":"2023-09-14T10:02:48.187014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"id":"fkhGQMSWHXWr","outputId":"7dacbdd2-1cfd-4a55-8daa-d1cc566ac66b","execution":{"iopub.status.busy":"2023-09-14T10:02:48.223165Z","iopub.execute_input":"2023-09-14T10:02:48.224071Z","iopub.status.idle":"2023-09-14T10:02:48.247667Z","shell.execute_reply.started":"2023-09-14T10:02:48.224017Z","shell.execute_reply":"2023-09-14T10:02:48.246677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Classes'].value_counts()","metadata":{"id":"mHhVwvJXHXWu","outputId":"1b322425-1d31-47c5-e272-0c198cec2a47","execution":{"iopub.status.busy":"2023-09-14T10:02:48.286937Z","iopub.execute_input":"2023-09-14T10:02:48.287333Z","iopub.status.idle":"2023-09-14T10:02:48.295752Z","shell.execute_reply.started":"2023-09-14T10:02:48.287301Z","shell.execute_reply":"2023-09-14T10:02:48.29488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Independent And dependent features","metadata":{}},{"cell_type":"code","source":"## Independent And dependent features\nX=df.drop('FWI',axis=1)\ny=df['FWI']","metadata":{"id":"Jdu-YiFjHXWw","execution":{"iopub.status.busy":"2023-09-14T10:02:48.373681Z","iopub.execute_input":"2023-09-14T10:02:48.374469Z","iopub.status.idle":"2023-09-14T10:02:48.382683Z","shell.execute_reply.started":"2023-09-14T10:02:48.37441Z","shell.execute_reply":"2023-09-14T10:02:48.381531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"id":"03ZOMamIHXWw","outputId":"012defe9-ca14-4c0c-9657-84211494b71a","execution":{"iopub.status.busy":"2023-09-14T10:02:48.401298Z","iopub.execute_input":"2023-09-14T10:02:48.402736Z","iopub.status.idle":"2023-09-14T10:02:48.422746Z","shell.execute_reply.started":"2023-09-14T10:02:48.402694Z","shell.execute_reply":"2023-09-14T10:02:48.420794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"id":"n1icCttKHXWx","outputId":"0e744e96-62be-4943-c9ac-aff45801b660","execution":{"iopub.status.busy":"2023-09-14T10:02:48.433723Z","iopub.execute_input":"2023-09-14T10:02:48.435533Z","iopub.status.idle":"2023-09-14T10:02:48.446155Z","shell.execute_reply.started":"2023-09-14T10:02:48.435471Z","shell.execute_reply":"2023-09-14T10:02:48.444659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"#Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)","metadata":{"id":"ERTXbiMIHXWx","execution":{"iopub.status.busy":"2023-09-14T10:02:48.460431Z","iopub.execute_input":"2023-09-14T10:02:48.461885Z","iopub.status.idle":"2023-09-14T10:02:48.469591Z","shell.execute_reply.started":"2023-09-14T10:02:48.461842Z","shell.execute_reply":"2023-09-14T10:02:48.468577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape,X_test.shape","metadata":{"id":"qFc-4MwUHXWy","outputId":"ba4ec477-79e4-4c40-e8eb-5585991eb46a","execution":{"iopub.status.busy":"2023-09-14T10:02:48.489798Z","iopub.execute_input":"2023-09-14T10:02:48.490455Z","iopub.status.idle":"2023-09-14T10:02:48.500022Z","shell.execute_reply.started":"2023-09-14T10:02:48.490396Z","shell.execute_reply":"2023-09-14T10:02:48.498999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation:\n\nCorrelation is like a math tool that helps us see if two things are connected.\n\nImagine you have data on two things: how much you study each day and how well you do on your exams. Correlation helps us figure out if studying more or less has anything to do with getting higher or lower scores on your tests.\n\n**There are two kinds of correlation:**\n\n#### 1. Positive Correlation:\nThis is when studying more is connected to getting better exam scores. So, if you spend more time studying, you usually get higher test scores. Think of it like this: more study, better grades.\n\n#### 2. Negative Correlation: \nHere, it's the opposite. If you study more, you tend to get lower scores, or if you study less, you get higher scores. This might mean that too much studying or too little studying isn't great for your test results.\n\n####  No Correlation: \nThis simply means that there's no clear link between how much you study and how well you do on exams. Studying more or less doesn't seem to consistently change your scores.\n\nUnderstanding correlation is helpful because it shows us patterns. For example:\n\n- If we find a positive correlation, it means spending more time studying could lead to better grades.\n\n- If there's a negative correlation, it might mean we need to find a balance between studying and other things to improve our scores.\n\nRemember, correlation doesn't prove that one thing causes another. It just tells us they're connected in some way. So, it's a useful tool for making smart decisions and seeing how things relate to each other.","metadata":{}},{"cell_type":"markdown","source":"## What is Feature Selection? \n\nImagine you're trying to cook a delicious meal, and you have a lot of ingredients in your kitchen. Some ingredients make your dish taste amazing, while others don't add much flavor. Feature selection is like choosing the best ingredients for your recipe.\n\nIn feature selection for a regression problem (predicting a number), you have a bunch of characteristics or features for each thing you're trying to predict. You want to pick the most important ones that actually help you make good predictions, without making things too complicated.\n\nHere are some ways to do this:\n\n1. Univariate Feature Selection: You look at each feature by itself and see how well it's connected to what you're trying to predict. If it's not very useful, you leave it out.\n\n2. Recursive Feature Elimination (RFE): You start with all your features and then keep removing the less important ones one by one until you're left with only the most important ones.\n\n3. LASSO: This is like giving a penalty to some features, making them less important. It encourages the model to ignore features that aren't very helpful.\n\n4. Feature Importance from Tree-based Models: You use a special kind of model to figure out which features have the most impact. It's like asking a tree which ingredients are the most important for your recipe.\n\n5. Correlation and Domain Knowledge: You check if some features are related to each other or if you already know some features are important based on what you know about the problem. This helps you choose the right ingredients based on common sense.\n\nSo, feature selection is like being a smart chef in the kitchen. You only use the ingredients that make your dish taste the best, and you leave out the ones that don't make a difference. This way, you end up with a simpler and tastier meal, or in this case, a better regression model.\n\n\n\n\nWe are using **Correlation and Domain Knowledge** technique for our problem","metadata":{}},{"cell_type":"markdown","source":"## Feature Selection based on correlaltion","metadata":{}},{"cell_type":"code","source":"## Feature Selection based on correlaltion\nX_train.corr()","metadata":{"id":"Z6p3lMgPHXWz","outputId":"a395bb8c-f0aa-4d4d-b2fe-341f648e7cd7","execution":{"iopub.status.busy":"2023-09-14T10:02:48.510599Z","iopub.execute_input":"2023-09-14T10:02:48.511278Z","iopub.status.idle":"2023-09-14T10:02:48.534869Z","shell.execute_reply.started":"2023-09-14T10:02:48.511244Z","shell.execute_reply":"2023-09-14T10:02:48.533557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observations:\n\n**Positive Correlation:**\n\nTemperature has a strong positive correlation with FFMC (0.69) and ISI (0.62). FFMC, DMC, DC, ISI, BUI, and FWI are positively correlated with each other, with correlation coefficients ranging from 0.58 to 0.76.\n\n**Negative Correlation:**\n\nTemperature has a strong negative correlation with RH (-0.65). RH is negatively correlated with FFMC, DMC, DC, ISI, BUI, and FWI, with correlation coefficients ranging from -0.68 to -0.58. Rain is negatively correlated with FFMC, DMC, DC, ISI, BUI, and FWI, but the correlation is relatively weak (between -0.37 and -0.04).\n\n**Weak Correlation:**\n\nThe correlation between Ws (wind speed) and other variables is weak, with coefficients ranging from -0.18 to 0.07. The correlation between the 'Region' variable and other variables is also weak, with coefficients ranging from -0.40 to 0.26.\n\nLets Visualize co relation using Heatmap given below","metadata":{}},{"cell_type":"markdown","source":"## Feature Selection","metadata":{"id":"gOIdY1kXHXW1"}},{"cell_type":"code","source":"## Check for multicollinearity\nplt.figure(figsize=(12,10))\ncorr=X_train.corr()\nsns.heatmap(corr,annot=True)\n","metadata":{"id":"tcx61AWpHXW6","outputId":"3d859daa-1f42-439e-ac54-c6fa53ac9818","execution":{"iopub.status.busy":"2023-09-14T10:02:48.538114Z","iopub.execute_input":"2023-09-14T10:02:48.538664Z","iopub.status.idle":"2023-09-14T10:02:49.392779Z","shell.execute_reply.started":"2023-09-14T10:02:48.538632Z","shell.execute_reply":"2023-09-14T10:02:49.391408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.corr()","metadata":{"id":"XV1bMRTMHXW7","outputId":"75b4fe9e-1927-423b-bbdf-45f328fb2e7c","execution":{"iopub.status.busy":"2023-09-14T10:02:49.395371Z","iopub.execute_input":"2023-09-14T10:02:49.395807Z","iopub.status.idle":"2023-09-14T10:02:49.422384Z","shell.execute_reply.started":"2023-09-14T10:02:49.395765Z","shell.execute_reply":"2023-09-14T10:02:49.421147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlation(dataset, threshold):\n    col_corr = set()\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold:\n                colname = corr_matrix.columns[i]\n                col_corr.add(colname)\n    return col_corr","metadata":{"id":"czVIwTiiHXW8","execution":{"iopub.status.busy":"2023-09-14T10:02:49.42382Z","iopub.execute_input":"2023-09-14T10:02:49.424396Z","iopub.status.idle":"2023-09-14T10:02:49.435025Z","shell.execute_reply.started":"2023-09-14T10:02:49.424336Z","shell.execute_reply":"2023-09-14T10:02:49.433397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## threshold--Domain expertise\ncorr_features=correlation(X_train,0.85)","metadata":{"id":"kJwbFU67HXW-","execution":{"iopub.status.busy":"2023-09-14T10:02:49.438821Z","iopub.execute_input":"2023-09-14T10:02:49.439268Z","iopub.status.idle":"2023-09-14T10:02:49.452069Z","shell.execute_reply.started":"2023-09-14T10:02:49.439235Z","shell.execute_reply":"2023-09-14T10:02:49.450225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have set the threshold to 0.85, we are looking for features that have a very strong correlation with each other. These highly correlated features can potentially redundant in our model, and removing one of them can help improve the our model's accuracy and make it easier to interpret the relationships between features and the target variable. The threshold is a way to identify which features are so strongly related that we might not need both of them in our regression model","metadata":{}},{"cell_type":"code","source":"corr_features","metadata":{"id":"mcpFStiQHXW_","outputId":"99317640-3961-4b13-d628-4576116bc1f7","execution":{"iopub.status.busy":"2023-09-14T10:02:49.454399Z","iopub.execute_input":"2023-09-14T10:02:49.454997Z","iopub.status.idle":"2023-09-14T10:02:49.469511Z","shell.execute_reply.started":"2023-09-14T10:02:49.454952Z","shell.execute_reply":"2023-09-14T10:02:49.467981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BUI' and 'DC' are highly correlated, they might be redundant in our regression model. To avoid multicollinearity issues and improve the model's performance, we decide to remove \"DC\" from one of these features.","metadata":{}},{"cell_type":"code","source":"## drop features when correlation is more than 0.85\nX_train.drop(corr_features,axis=1,inplace=True)\nX_test.drop(corr_features,axis=1,inplace=True)\nX_train.shape,X_test.shape","metadata":{"id":"1FGA5zRmHXXA","outputId":"63b5ee7e-6098-4e28-b385-c2ebef8baa6e","execution":{"iopub.status.busy":"2023-09-14T10:02:49.47204Z","iopub.execute_input":"2023-09-14T10:02:49.472531Z","iopub.status.idle":"2023-09-14T10:02:49.490314Z","shell.execute_reply.started":"2023-09-14T10:02:49.47249Z","shell.execute_reply":"2023-09-14T10:02:49.488904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multicollinearity\n\n\nMulticollinearity is like having two or more ingredients in a recipe that taste almost the same or do similar things. It's when some features in your data are so related that it confuses a regression model, like having two chefs doing the same cooking task together, and you can't tell whose effort made the dish taste a certain way.\n\nFor example, let's say you have two ingredients, 'BUI' and 'DC,' and they are very similar in how they affect your dish. They have a correlation score of 0.85, which is higher than a threshold you set. This means they are strongly connected.\n\nNow, if you use both 'BUI' and 'DC' in your recipe (or regression model), it's like having two chefs in the kitchen doing the same job. The model can't figure out who is more responsible for the final taste (or the target variable).\n\nTo avoid this confusion, you decide to remove one of these very similar ingredients, 'DC,' from your recipe. By doing this, you make sure that the model can focus on the unique contribution of 'BUI' without getting mixed up by the similar effect of 'DC.' It's like having just one chef doing the cooking job, so you can clearly see the impact of each ingredient.","metadata":{}},{"cell_type":"markdown","source":"## What is Feature Scaling \n\n**Feature scaling** is the technique to bring all the features to the same scale. If we don‚Äôt bring the features to the same scale, the model tends to give higher weightage to higher values and lower weightage to lower values irrespective of the units of values. \nFeature scaling is bringing continuous variables to the same scale. \n\nFor example, student A got 60 out of 100 in subject 1, 120 out of 150 in subject 2, 180 out of 200 in subject 3.\n\nAfter rescaling to 10, Student A got 6 out of 10 in subject 1, 8 out of 10 in subject 2, 9 out of 10 in subject 3\n\n\n### Why Feature Scaling\n\nMachine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. The difference in ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.","metadata":{}},{"cell_type":"markdown","source":"## What is Standardization\nIt is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n$$X_{new}=\\frac{x_i-X_{mean}}{u}$$","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:33:47.498147Z","iopub.execute_input":"2023-09-14T08:33:47.502084Z","iopub.status.idle":"2023-09-14T08:33:47.508009Z","shell.execute_reply.started":"2023-09-14T08:33:47.502035Z","shell.execute_reply":"2023-09-14T08:33:47.506602Z"}}},{"cell_type":"markdown","source":"## Implementing Feature Scaling Or Standardization using Sklearn","metadata":{"id":"uYt41ACIHXXA"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nX_train_scaled=scaler.fit_transform(X_train)\nX_test_scaled=scaler.transform(X_test)","metadata":{"id":"p3VVEAVeHXXB","execution":{"iopub.status.busy":"2023-09-14T10:02:49.492208Z","iopub.execute_input":"2023-09-14T10:02:49.492715Z","iopub.status.idle":"2023-09-14T10:02:49.509313Z","shell.execute_reply.started":"2023-09-14T10:02:49.492662Z","shell.execute_reply":"2023-09-14T10:02:49.508144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_scaled","metadata":{"id":"5x6sfm5_HXXC","outputId":"96c34ec6-5b44-439e-9de4-a204be0fb7f6","execution":{"iopub.status.busy":"2023-09-14T10:02:49.51107Z","iopub.execute_input":"2023-09-14T10:02:49.511524Z","iopub.status.idle":"2023-09-14T10:02:49.522986Z","shell.execute_reply.started":"2023-09-14T10:02:49.511485Z","shell.execute_reply":"2023-09-14T10:02:49.521352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Box Plots To understand Effect Of Standard Scaler","metadata":{"id":"99bItflhHXXC"}},{"cell_type":"code","source":"plt.subplots(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.boxplot(data=X_train)\nplt.title('X_train Before Scaling')\nplt.subplot(1, 2, 2)\nsns.boxplot(data=X_train_scaled)\nplt.title('X_train After Scaling')","metadata":{"id":"xrAONEtmHXXC","outputId":"883e5de6-8fb1-48c6-c83c-7fe04cbb429a","execution":{"iopub.status.busy":"2023-09-14T10:02:49.524535Z","iopub.execute_input":"2023-09-14T10:02:49.525306Z","iopub.status.idle":"2023-09-14T10:02:50.257406Z","shell.execute_reply.started":"2023-09-14T10:02:49.525268Z","shell.execute_reply":"2023-09-14T10:02:50.255906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Before Scaling (Left Plot):** The left plot shows the distribution of the features in the original training dataset (X_train) before scaling. Each feature is represented by a box, and the box contains a range of values. The box represents the interquartile range (IQR), which is the middle 50% of the data. The line inside the box represents the median value, and the \"whiskers\" extending from the box show the minimum and maximum values within a certain range. Any points beyond the whiskers are considered outliers.\n\nThis plot allows us to see how the data is spread out for each feature. If the boxes are narrow, it means the data values are close together, and if they are wide, it means the data values are more spread out.\n\n**After Scaling (Right Plot):** The right plot shows the distribution of the features in the training dataset (X_train_scaled) after scaling. The features have been scaled to have zero mean and unit variance.\n\nBy scaling the features, we make sure that each feature contributes equally to the model. It also helps certain machine learning algorithms converge faster and improves model performance.","metadata":{}},{"cell_type":"markdown","source":"## Performance Metrics\n\n**Mean Absolute Error (MAE):**\nThe Mean Absolute Error is a measure of how well a model predicts the actual values. It calculates the average difference between the predicted values and the actual values, ignoring whether the differences are positive or negative. In other words, it measures how far, on average, the predicted values are from the actual values. In simpler terms, the MAE tells us, on average, how far off our predictions are from the actual values.\n\nFor example, suppose we are predicting house prices, and the MAE is 1.7. This means that, on average, our predictions differ from the actual house prices by $1.7k. The lower the MAE, the better the model's predictions are, as it means the model is making more accurate predictions.\n\n**R2 Score:**\nThe R2 Score, also known as the coefficient of determination, is a measure of how well the model explains the variance in the data. It ranges from 0 to 1, where 0 indicates that the model does not explain any variance, and 1 means the model perfectly explains the variance.\n\nIn simpler terms, the R2 Score tells us how much of the variation in the actual values is captured by the model's predictions. A higher R2 Score (closer to 1) indicates that the model is doing a good job of explaining the variability in the data, and its predictions are in line with the actual values.\n\nFor example, if the R2 Score is 0.90, it means that about 90% of the variation in the actual values can be explained by the model's predictions. This is a good R2 Score, showing that the model is capturing a significant amount of the data's variability and making accurate predictions.","metadata":{}},{"cell_type":"markdown","source":"## Linear Regression Model","metadata":{"id":"BD0gDnWtHXXD"}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nlinreg=LinearRegression()\nlinreg.fit(X_train_scaled,y_train)\ny_pred=linreg.predict(X_test_scaled)\nmae=mean_absolute_error(y_test,y_pred)\nscore=r2_score(y_test,y_pred)\nprint(\"Mean absolute error: \", mae)\nprint(\"R2 Score: \", score)","metadata":{"id":"w8iysLQkHXXD","outputId":"2be34b0a-e3ca-48a4-e7d2-180dd7891310","execution":{"iopub.status.busy":"2023-09-14T10:02:50.26281Z","iopub.execute_input":"2023-09-14T10:02:50.263225Z","iopub.status.idle":"2023-09-14T10:02:50.277423Z","shell.execute_reply.started":"2023-09-14T10:02:50.263195Z","shell.execute_reply":"2023-09-14T10:02:50.275859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lasso Regression","metadata":{"id":"vm94UHqjHXXE"}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nlasso=Lasso()\nlasso.fit(X_train_scaled,y_train)\ny_pred=lasso.predict(X_test_scaled)\nmae=mean_absolute_error(y_test,y_pred)\nscore=r2_score(y_test,y_pred)\nprint(\"Mean absolute error: \", mae)\nprint(\"R2 Score: \", score)","metadata":{"id":"hLUErVQhHXXE","outputId":"596c1db8-b9cb-40ca-9eb3-b699dcb7c743","execution":{"iopub.status.busy":"2023-09-14T10:02:50.279433Z","iopub.execute_input":"2023-09-14T10:02:50.280105Z","iopub.status.idle":"2023-09-14T10:02:50.294749Z","shell.execute_reply.started":"2023-09-14T10:02:50.280055Z","shell.execute_reply":"2023-09-14T10:02:50.29298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ridge Regression model","metadata":{"id":"uUAoi2ZEHXXE"}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nridge=Ridge()\nridge.fit(X_train_scaled,y_train)\ny_pred=ridge.predict(X_test_scaled)\nmae=mean_absolute_error(y_test,y_pred)\nscore=r2_score(y_test,y_pred)\nprint(\"Mean absolute error: \", mae)\nprint(\"R2 Score: \", score)","metadata":{"id":"MQCU9YlJHXXF","outputId":"1a2ea591-d946-4c46-882a-572e1ee15e74","execution":{"iopub.status.busy":"2023-09-14T10:02:50.29659Z","iopub.execute_input":"2023-09-14T10:02:50.297035Z","iopub.status.idle":"2023-09-14T10:02:50.320984Z","shell.execute_reply.started":"2023-09-14T10:02:50.296999Z","shell.execute_reply":"2023-09-14T10:02:50.319614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Elasticnet Regression","metadata":{"id":"Jt48VIL4HXXF"}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nelastic=ElasticNet()\nelastic.fit(X_train_scaled,y_train)\ny_pred=elastic.predict(X_test_scaled)\nmae=mean_absolute_error(y_test,y_pred)\nscore=r2_score(y_test,y_pred)\nprint(\"Mean absolute error: \", mae)\nprint(\"R2 Score: \", score)","metadata":{"id":"QrsGK5DGHXXF","outputId":"79527b1f-6a24-4152-ce29-d4b210e296aa","execution":{"iopub.status.busy":"2023-09-14T10:02:50.323516Z","iopub.execute_input":"2023-09-14T10:02:50.324157Z","iopub.status.idle":"2023-09-14T10:02:50.345491Z","shell.execute_reply.started":"2023-09-14T10:02:50.32412Z","shell.execute_reply":"2023-09-14T10:02:50.343924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Top Performing models¬∂\n**Ridge Regression:**\n\n- Mean Absolute Error (MAE): 0.498\n- R2 Score: 0.988\n\n**Linear Regression:**\n\n- Mean Absolute Error (MAE): 0.482\n- R2 Score: 0.989","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\nIn both cases, the R2 Score is quite high, which means that both models do a good job of explaining the variation in the data and making accurate predictions.\n\nHowever, when it comes to the Mean Absolute Error (MAE), we can see that Linear Regression has a slightly lower value (0.482) compared to Ridge Regression (0.498).\n\nConsidering both the R2 Score and MAE, we can conclude that the Linear Regression model performs slightly better than the Ridge Regression model in terms of prediction accuracy. \nbut **We would prefer using the Ridge Regression model for making predictions because Ridge Regression can effectively address the problem of overfitting.**","metadata":{}},{"cell_type":"markdown","source":"## Pickling\nPython pickle module is used for serialising and de-serialising a Python object structure. Any object in Python can be pickled so that it can be saved on disk. What pickle does is that it ‚Äúserialises‚Äù the object first before writing it to file. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script.","metadata":{}},{"cell_type":"code","source":"import pickle\npickle.dump(scaler,open('scaler.pkl','wb'))\npickle.dump(ridge,open('ridge.pkl','wb'))","metadata":{"id":"BQllvjZYHXXG","execution":{"iopub.status.busy":"2023-09-14T10:02:50.346924Z","iopub.execute_input":"2023-09-14T10:02:50.348296Z","iopub.status.idle":"2023-09-14T10:02:50.359776Z","shell.execute_reply.started":"2023-09-14T10:02:50.348239Z","shell.execute_reply":"2023-09-14T10:02:50.35857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thank you for checking out notebook!\n\nThat concludes our exploration in this Jupyter Notebook. If you found this notebook helpful or insightful, I would greatly appreciate it if you could **upvote it.**\n\nThank you once again for your time and consideration. If you have any feedback or suggestions, feel free to leave a comment. I'm always open to learning and improving.\n\n \n \n \n \n\nThanks & Regards,\n\nSudhanshu Kumar","metadata":{"id":"bJhIvZzjHXXG"}}]}